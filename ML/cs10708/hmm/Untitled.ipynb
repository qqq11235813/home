{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/python\n",
    "\n",
    "__author__=\"Daniel Bauer <bauer@cs.columbia.edu>\"\n",
    "__date__ =\"$Sep 12, 2011\"\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "Count n-gram frequencies in a data file and write counts to\n",
    "stdout. \n",
    "\"\"\"\n",
    "\n",
    "def simple_conll_corpus_iterator(corpus_file):\n",
    "    \"\"\"\n",
    "    Get an iterator object over the corpus file. The elements of the\n",
    "    iterator contain (word, ne_tag) tuples. Blank lines, indicating\n",
    "    sentence boundaries return (None, None).\n",
    "    \"\"\"\n",
    "    l = corpus_file.readline()\n",
    "    while l:\n",
    "        line = l.strip()\n",
    "        if line: # Nonempty line\n",
    "            # Extract information from line.\n",
    "            # Each line has the format\n",
    "            # word pos_tag phrase_tag ne_tag\n",
    "            fields = line.split(\" \")\n",
    "            ne_tag = fields[-1]\n",
    "            #phrase_tag = fields[-2] #Unused\n",
    "            #pos_tag = fields[-3] #Unused\n",
    "            word = \" \".join(fields[:-1])\n",
    "            yield word, ne_tag\n",
    "        else: # Empty line\n",
    "            yield (None, None)                        \n",
    "        l = corpus_file.readline()\n",
    "\n",
    "def sentence_iterator(corpus_iterator):\n",
    "    \"\"\"\n",
    "    Return an iterator object that yields one sentence at a time.\n",
    "    Sentences are represented as lists of (word, ne_tag) tuples.\n",
    "    \"\"\"\n",
    "    current_sentence = [] #Buffer for the current sentence\n",
    "    for l in corpus_iterator:        \n",
    "            if l==(None, None):\n",
    "                if current_sentence:  #Reached the end of a sentence\n",
    "                    yield current_sentence\n",
    "                    current_sentence = [] #Reset buffer\n",
    "                else: # Got empty input stream\n",
    "                    sys.stderr.write(\"WARNING: Got empty input file/stream.\\n\")\n",
    "                    raise StopIteration\n",
    "            else:\n",
    "                current_sentence.append(l) #Add token to the buffer\n",
    "\n",
    "    if current_sentence: # If the last line was blank, we're done\n",
    "        yield current_sentence  #Otherwise when there is no more token\n",
    "                                # in the stream return the last sentence.\n",
    "\n",
    "def get_ngrams(sent_iterator, n):\n",
    "    \"\"\"\n",
    "    Get a generator that returns n-grams over the entire corpus,\n",
    "    respecting sentence boundaries and inserting boundary tokens.\n",
    "    Sent_iterator is a generator object whose elements are lists\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    for sent in sent_iterator:\n",
    "         #Add boundary symbols to the sentence\n",
    "         w_boundary = (n-1) * [(None, \"*\")]\n",
    "         w_boundary.extend(sent)\n",
    "         w_boundary.append((None, \"STOP\"))\n",
    "         #Then extract n-grams\n",
    "         ngrams = (tuple(w_boundary[i:i+n]) for i in xrange(len(w_boundary)-n+1))\n",
    "         for n_gram in ngrams: #Return one n-gram at a time\n",
    "            yield n_gram        \n",
    "\n",
    "\n",
    "class Hmm(object):\n",
    "    \"\"\"\n",
    "    Stores counts for n-grams and emissions. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=3):\n",
    "        assert n>=2, \"Expecting n>=2.\"\n",
    "        self.n = n\n",
    "        self.emission_counts = defaultdict(int)\n",
    "        self.ngram_counts = [defaultdict(int) for i in xrange(self.n)]\n",
    "        self.all_states = set()\n",
    "\n",
    "    def train(self, corpus_file):\n",
    "        \"\"\"\n",
    "        Count n-gram frequencies and emission probabilities from a corpus file.\n",
    "        \"\"\"\n",
    "        ngram_iterator = \\\n",
    "            get_ngrams(sentence_iterator(simple_conll_corpus_iterator(corpus_file)), self.n)\n",
    "\n",
    "        for ngram in ngram_iterator:\n",
    "            #Sanity check: n-gram we get from the corpus stream needs to have the right length\n",
    "            assert len(ngram) == self.n, \"ngram in stream is %i, expected %i\" % (len(ngram, self.n))\n",
    "\n",
    "            tagsonly = tuple([ne_tag for word, ne_tag in ngram]) #retrieve only the tags            \n",
    "            for i in xrange(2, self.n+1): #Count NE-tag 2-grams..n-grams\n",
    "                self.ngram_counts[i-1][tagsonly[-i:]] += 1\n",
    "            \n",
    "            if ngram[-1][0] is not None: # If this is not the last word in a sentence\n",
    "                self.ngram_counts[0][tagsonly[-1:]] += 1 # count 1-gram\n",
    "                self.emission_counts[ngram[-1]] += 1 # and emission frequencies\n",
    "\n",
    "            # Need to count a single n-1-gram of sentence start symbols per sentence\n",
    "            if ngram[-2][0] is None: # this is the first n-gram in a sentence\n",
    "                self.ngram_counts[self.n - 2][tuple((self.n - 1) * [\"*\"])] += 1\n",
    "\n",
    "    def write_counts(self, output, printngrams=[1,2,3]):\n",
    "        \"\"\"\n",
    "        Writes counts to the output file object.\n",
    "        Format:\n",
    "\n",
    "        \"\"\"\n",
    "        # First write counts for emissions\n",
    "        for word, ne_tag in self.emission_counts:            \n",
    "            output.write(\"%i WORDTAG %s %s\\n\" % (self.emission_counts[(word, ne_tag)], ne_tag, word))\n",
    "\n",
    "\n",
    "        # Then write counts for all ngrams\n",
    "        for n in printngrams:            \n",
    "            for ngram in self.ngram_counts[n-1]:\n",
    "                ngramstr = \" \".join(ngram)\n",
    "                output.write(\"%i %i-GRAM %s\\n\" %(self.ngram_counts[n-1][ngram], n, ngramstr))\n",
    "\n",
    "    def read_counts(self, corpusfile):\n",
    "\n",
    "        self.n = 3\n",
    "        self.emission_counts = defaultdict(int)\n",
    "        self.ngram_counts = [defaultdict(int) for i in xrange(self.n)]\n",
    "        self.all_states = set()\n",
    "\n",
    "        for line in corpusfile:\n",
    "            parts = line.strip().split(\" \")\n",
    "            count = float(parts[0])\n",
    "            if parts[1] == \"WORDTAG\":\n",
    "                ne_tag = parts[2]\n",
    "                word = parts[3]\n",
    "                self.emission_counts[(word, ne_tag)] = count\n",
    "                self.all_states.add(ne_tag)\n",
    "            elif parts[1].endswith(\"GRAM\"):\n",
    "                n = int(parts[1].replace(\"-GRAM\",\"\"))\n",
    "                ngram = tuple(parts[2:])\n",
    "                self.ngram_counts[n-1][ngram] = count\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-327ac040aaff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8e974468a363>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memission_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xrange' is not defined"
     ]
    }
   ],
   "source": [
    "filename=\"gene.train\"\n",
    "input=open(filename,\"r\")\n",
    "\n",
    "count=Hmm(3)\n",
    "count.train(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
