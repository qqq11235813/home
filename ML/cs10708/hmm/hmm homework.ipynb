{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python count_freqs.py [input_file] > [output_file]Read in a gene tagged training input file and produce counts.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/python\n",
    "\n",
    "__author__=\"Daniel Bauer <bauer@cs.columbia.edu>\"\n",
    "__date__ =\"$Sep 12, 2011\"\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "Count n-gram frequencies in a data file and write counts to\n",
    "stdout. \n",
    "\"\"\"\n",
    "\n",
    "def simple_conll_corpus_iterator(corpus_file):\n",
    "    \"\"\"\n",
    "    Get an iterator object over the corpus file. The elements of the\n",
    "    iterator contain (word, ne_tag) tuples. Blank lines, indicating\n",
    "    sentence boundaries return (None, None).\n",
    "    \"\"\"\n",
    "    l = corpus_file.readline()\n",
    "    while l:\n",
    "        line = l.strip()\n",
    "        if line: # Nonempty line\n",
    "            # Extract information from line.\n",
    "            # Each line has the format\n",
    "            # word pos_tag phrase_tag ne_tag\n",
    "            fields = line.split(\" \")\n",
    "            ne_tag = fields[-1]\n",
    "            #phrase_tag = fields[-2] #Unused\n",
    "            #pos_tag = fields[-3] #Unused\n",
    "            word = \" \".join(fields[:-1])\n",
    "            yield word, ne_tag\n",
    "        else: # Empty line\n",
    "            yield (None, None)                        \n",
    "        l = corpus_file.readline()\n",
    "\n",
    "def sentence_iterator(corpus_iterator):\n",
    "    \"\"\"\n",
    "    Return an iterator object that yields one sentence at a time.\n",
    "    Sentences are represented as lists of (word, ne_tag) tuples.\n",
    "    \"\"\"\n",
    "    current_sentence = [] #Buffer for the current sentence\n",
    "    for l in corpus_iterator:        \n",
    "            if l==(None, None):\n",
    "                if current_sentence:  #Reached the end of a sentence\n",
    "                    yield current_sentence\n",
    "                    current_sentence = [] #Reset buffer\n",
    "                else: # Got empty input stream\n",
    "                    sys.stderr.write(\"WARNING: Got empty input file/stream.\\n\")\n",
    "                    raise StopIteration\n",
    "            else:\n",
    "                current_sentence.append(l) #Add token to the buffer\n",
    "\n",
    "    if current_sentence: # If the last line was blank, we're done\n",
    "        yield current_sentence  #Otherwise when there is no more token\n",
    "                                # in the stream return the last sentence.\n",
    "\n",
    "def get_ngrams(sent_iterator, n):\n",
    "    \"\"\"\n",
    "    Get a generator that returns n-grams over the entire corpus,\n",
    "    respecting sentence boundaries and inserting boundary tokens.\n",
    "    Sent_iterator is a generator object whose elements are lists\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    for sent in sent_iterator:\n",
    "         #Add boundary symbols to the sentence\n",
    "         w_boundary = (n-1) * [(None, \"*\")]\n",
    "         w_boundary.extend(sent)\n",
    "         w_boundary.append((None, \"STOP\"))\n",
    "         #Then extract n-grams\n",
    "         ngrams = (tuple(w_boundary[i:i+n]) for i in xrange(len(w_boundary)-n+1))\n",
    "         for n_gram in ngrams: #Return one n-gram at a time\n",
    "            yield n_gram        \n",
    "\n",
    "\n",
    "class Hmm(object):\n",
    "    \"\"\"\n",
    "    Stores counts for n-grams and emissions. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=3):\n",
    "        assert n>=2, \"Expecting n>=2.\"\n",
    "        self.n = n\n",
    "        self.emission_counts = defaultdict(int)\n",
    "        self.ngram_counts = [defaultdict(int) for i in xrange(self.n)]\n",
    "        self.all_states = set()\n",
    "\n",
    "    def train(self, corpus_file):\n",
    "        \"\"\"\n",
    "        Count n-gram frequencies and emission probabilities from a corpus file.\n",
    "        \"\"\"\n",
    "        ngram_iterator = \\\n",
    "            get_ngrams(sentence_iterator(simple_conll_corpus_iterator(corpus_file)), self.n)\n",
    "\n",
    "        for ngram in ngram_iterator:\n",
    "            #Sanity check: n-gram we get from the corpus stream needs to have the right length\n",
    "            assert len(ngram) == self.n, \"ngram in stream is %i, expected %i\" % (len(ngram, self.n))\n",
    "\n",
    "            tagsonly = tuple([ne_tag for word, ne_tag in ngram]) #retrieve only the tags            \n",
    "            for i in xrange(2, self.n+1): #Count NE-tag 2-grams..n-grams\n",
    "                self.ngram_counts[i-1][tagsonly[-i:]] += 1\n",
    "            \n",
    "            if ngram[-1][0] is not None: # If this is not the last word in a sentence\n",
    "                self.ngram_counts[0][tagsonly[-1:]] += 1 # count 1-gram\n",
    "                self.emission_counts[ngram[-1]] += 1 # and emission frequencies\n",
    "\n",
    "            # Need to count a single n-1-gram of sentence start symbols per sentence\n",
    "            if ngram[-2][0] is None: # this is the first n-gram in a sentence\n",
    "                self.ngram_counts[self.n - 2][tuple((self.n - 1) * [\"*\"])] += 1\n",
    "\n",
    "    def write_counts(self, output, printngrams=[1,2,3]):\n",
    "        \"\"\"\n",
    "        Writes counts to the output file object.\n",
    "        Format:\n",
    "\n",
    "        \"\"\"\n",
    "        # First write counts for emissions\n",
    "        for word, ne_tag in self.emission_counts:            \n",
    "            output.write(\"%i WORDTAG %s %s\\n\" % (self.emission_counts[(word, ne_tag)], ne_tag, word))\n",
    "\n",
    "\n",
    "        # Then write counts for all ngrams\n",
    "        for n in printngrams:            \n",
    "            for ngram in self.ngram_counts[n-1]:\n",
    "                ngramstr = \" \".join(ngram)\n",
    "                output.write(\"%i %i-GRAM %s\\n\" %(self.ngram_counts[n-1][ngram], n, ngramstr))\n",
    "\n",
    "    def read_counts(self, corpusfile):\n",
    "\n",
    "        self.n = 3\n",
    "        self.emission_counts = defaultdict(int)\n",
    "        self.ngram_counts = [defaultdict(int) for i in xrange(self.n)]\n",
    "        self.all_states = set()\n",
    "\n",
    "        for line in corpusfile:\n",
    "            parts = line.strip().split(\" \")\n",
    "            count = float(parts[0])\n",
    "            if parts[1] == \"WORDTAG\":\n",
    "                ne_tag = parts[2]\n",
    "                word = parts[3]\n",
    "                self.emission_counts[(word, ne_tag)] = count\n",
    "                self.all_states.add(ne_tag)\n",
    "            elif parts[1].endswith(\"GRAM\"):\n",
    "                n = int(parts[1].replace(\"-GRAM\",\"\"))\n",
    "                ngram = tuple(parts[2:])\n",
    "                self.ngram_counts[n-1][ngram] = count\n",
    "                \n",
    "\n",
    "\n",
    "def usage():\n",
    "    print (\"python count_freqs.py [input_file] > [output_file]Read in a gene tagged training input file and produce counts.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if len(sys.argv)!=2: # Expect exactly one argument: the training data file\n",
    "        usage()\n",
    "        sys.exit(2)\n",
    "\n",
    "    try:\n",
    "        input = file(sys.argv[1],\"r\")\n",
    "    except IOError:\n",
    "        sys.stderr.write(\"ERROR: Cannot read inputfile %s.\\n\" % arg)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Initialize a trigram counter\n",
    "    counter = Hmm(3)\n",
    "    # Collect counts\n",
    "    counter.train(input)\n",
    "    # Write the counts\n",
    "    counter.write_counts(sys.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-cee9ee6bffca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gene.train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file' is not defined"
     ]
    }
   ],
   "source": [
    "filename=\"gene.train\"\n",
    "input=file(filename,\"r\")\n",
    "\n",
    "count=Hmm(3)\n",
    "count.train(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c84f63ba588f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnewCounts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memission_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memission_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"O\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memission_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mnewCounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"RARE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memission_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mnewCounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count' is not defined"
     ]
    }
   ],
   "source": [
    "newCounts=count.emission_counts\n",
    "for word,tag in count.emission_counts:\n",
    "    if((tag==\"O\")&(count.emission_counts[(word,tag)]<5)):\n",
    "        newCounts[(word,\"RARE\")]=count.emission_counts[(word,tag)]    \n",
    "        del newCounts[(word,tag)]\n",
    "count.emission_counts=newCounts\n",
    "\n",
    "wordBag={};wordBagProb={}\n",
    "for word,tag in count.emission_counts:\n",
    "    wordBag[tag]=0\n",
    "for word,tag in count.emission_counts:\n",
    "    wordBag[tag]+=(count.emission_counts[(word,tag)])\n",
    "for word,tag in count.emission_counts:\n",
    "    wordBagProb[(word,tag)]=count.emission_counts[(word,tag)]/(wordBag[tag]+0.0)\n",
    "#print wordBagProb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dad95b8e406d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mngram_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mngram_first\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"calculate the transfer probability of hidden tag\"\"\"\n",
    "ngram_first={}\n",
    "trans={}\n",
    "for ngram in count.ngram_counts[1]:\n",
    "    ngram_first[ngram[0]]=0\n",
    "\n",
    "for ngram in count.ngram_counts[1]:\n",
    "    ngram_first[ngram[0]]+=(count.ngram_counts[1][ngram])\n",
    "    \n",
    "print (ngram_first)\n",
    "\n",
    "for ngram  in count.ngram_counts[1]:\n",
    "    ngramstr = \" \".join(ngram)\n",
    "    trans[ngram]=count.ngram_counts[1][ngram]/(\n",
    "        0.0+ngram_first[ngram[0]])\n",
    "print (trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('I-GENE', 'I-GENE'): 41072, ('O', 'I-GENE'): 345128, ('I-GENE', 'O'): 41072, ('O', 'STOP'): 345128, ('I-GENE', 'STOP'): 41072, ('*', 'O'): 0, ('O', 'O'): 345128, ('*', 'I-GENE'): 0, ('*', '*'): 0}\n",
      "defaultdict(<type 'int'>, {('*',): 0, ('I-GENE',): 41072, ('O',): 345128})\n"
     ]
    }
   ],
   "source": [
    "transfer={}\n",
    "for tag1,tag2 in count.ngram_counts[1]:\n",
    "    transfer[(tag1,tag2)]=count.ngram_counts[0][(tag1,)]\n",
    "print transfer\n",
    "print count.ngram_counts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {('*',): 0, ('I-GENE',): 41072, ('O',): 345128})\n"
     ]
    }
   ],
   "source": [
    "print count.ngram_counts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
